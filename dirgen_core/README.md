# üß† DirGen Core - Servicios Centralizados de LLM

**DirGen Core** es el m√≥dulo central de la plataforma que proporciona servicios compartidos optimizados para la gesti√≥n de m√∫ltiples proveedores de LLM (Large Language Models). Implementa principios SOLID y patrones de dise√±o avanzados para garantizar escalabilidad, resiliencia y optimizaci√≥n de costos.

## üéØ Objetivos

- **üèóÔ∏è Modularidad**: Servicios LLM reutilizables y desacoplados
- **üí∞ Optimizaci√≥n de Costos**: Cache inteligente y consolidaci√≥n de historial
- **üõ°Ô∏è Resiliencia**: Fallback autom√°tico y manejo de errores robusto
- **‚ö° Performance**: Selecci√≥n √≥ptima de modelos seg√∫n tipo de tarea
- **üîÑ Escalabilidad**: Soporte para m√∫ltiples proveedores simult√°neos

## üì¶ Arquitectura del M√≥dulo

```
dirgen_core/
‚îú‚îÄ‚îÄ __init__.py                 # Punto de entrada del m√≥dulo
‚îî‚îÄ‚îÄ llm_services/              # Servicios especializados de LLM
    ‚îú‚îÄ‚îÄ __init__.py            # Exportaci√≥n de funciones principales
    ‚îú‚îÄ‚îÄ main_llm_service.py    # üéØ Servicio principal con l√≥gica de priorizaci√≥n
    ‚îú‚îÄ‚îÄ api_clients.py         # üîå Clientes para todos los proveedores LLM
    ‚îú‚îÄ‚îÄ local_model_manager.py # üñ•Ô∏è Gesti√≥n de modelos locales v√≠a Docker
    ‚îî‚îÄ‚îÄ gemini_key_rotator.py  # üîÑ Sistema de rotaci√≥n de claves API Gemini
```

## üöÄ Uso Principal

### Importaci√≥n Simplificada

```python
from dirgen_core.llm_services import ask_llm, get_agent_profile, select_optimal_model

# Llamada b√°sica
response = ask_llm(
    model_id="ai/gemma3-qat",
    system_prompt="Eres un arquitecto de software experto",
    user_prompt="Dise√±a una API REST para un sistema financiero",
    task_type="architecture",
    use_cache=True
)

# Obtener perfil de agente desde PCCE
profile = get_agent_profile(pcce_data, "planner")

# Selecci√≥n √≥ptima de modelo
optimal_model = select_optimal_model("complex_generation", profile)
```

## üîß Componentes Principales

### 1. üéØ `main_llm_service.py`

**Funci√≥n principal:** `ask_llm(model_id, system_prompt, user_prompt, task_type, use_cache)`

#### Caracter√≠sticas:
- **Priorizaci√≥n inteligente** de proveedores LLM
- **Fallback autom√°tico** en caso de fallos
- **Cache inteligente** para tareas repetitivas
- **Rate limit detection** con candado de seguridad
- **Selecci√≥n √≥ptima** de modelos por tipo de tarea

#### Tipos de tarea soportados:
- `planning`: Planificaci√≥n estrat√©gica
- `architecture`: Dise√±o de arquitectura
- `complex_generation`: Generaci√≥n de c√≥digo complejo
- `simple_generation`: Tareas simples
- `verification`: Validaci√≥n y verificaci√≥n
- `general`: Tareas generales

#### Configuraci√≥n:
```bash
# Variables de entorno
export LLM_PRIORITY_ORDER="gemini,local,groq,openai,anthropic,xai"
export OPENAI_API_KEY="tu-key"
export GEMINI_API_KEY="tu-key"
# ... otros proveedores
```

### 2. üîå `api_clients.py`

**Funciones disponibles:**
- `call_openai_llm(messages)` - Cliente OpenAI GPT
- `call_anthropic_llm(messages)` - Cliente Anthropic Claude  
- `call_groq_llm(messages)` - Cliente Groq
- `call_gemini_llm(messages)` - Cliente Google Gemini
- `call_xai_llm(messages)` - Cliente xAI Grok
- `call_local_llm(model_id, messages)` - Cliente para modelos locales

#### Caracter√≠sticas:
- **Manejo robusto de errores** con reintentos
- **Rate limiting** inteligente
- **Timeouts configurables**
- **Logging estructurado** para debugging

### 3. üñ•Ô∏è `local_model_manager.py`

**Funci√≥n principal:** `ensure_model_available(model_id)`

#### Modelos locales soportados:
- `ai/qwen3-coder` - Generaci√≥n de c√≥digo especializada
- `ai/gemma3-qat` - Tareas generales optimizadas  
- `ai/smollm3` - Tareas simples de bajo costo
- `ai/gemma3` - Tareas generales est√°ndar
- `ai/gemma3n` - Variante optimizada

#### Configuraci√≥n DMR:
```bash
export DMR_BASE_URL="http://localhost:8080"  # Dynamic Model Router
```

### 4. üîÑ `gemini_key_rotator.py`

**Funciones principales:**
- `get_rotated_gemini_key()` - Obtiene key con load balancing
- `record_gemini_result(key, success)` - Registra resultado de uso

#### Configuraci√≥n m√∫ltiples keys:
```bash
export GEMINI_API_KEY_1="key1"
export GEMINI_API_KEY_2="key2"  
export GEMINI_API_KEY_3="key3"
```

#### Caracter√≠sticas:
- **Load balancing** equitativo
- **Recovery autom√°tico** de keys bloqueadas
- **Estad√≠sticas de uso** por key
- **Rotaci√≥n inteligente** basada en performance

## üìä Optimizaci√≥n de Costos

### Cache Inteligente
```python
# Cache autom√°tico para tareas repetitivas
response = ask_llm(
    model_id="ai/smollm3",
    system_prompt="Sistema de validaci√≥n",
    user_prompt="Valida este JSON...",
    task_type="verification",
    use_cache=True  # ‚ú® Evita llamadas redundantes
)
```

### Selecci√≥n √ìptima de Modelos
- **Tareas simples** ‚Üí Modelos locales (costo = $0)
- **Tareas complejas** ‚Üí Modelos cloud (costo optimizado)
- **Verificaciones** ‚Üí Cache + modelos eficientes

## üõ°Ô∏è Resiliencia y Manejo de Errores

### Fallback Autom√°tico
```python
# Si Gemini falla ‚Üí Local ‚Üí Groq ‚Üí OpenAI ‚Üí Anthropic ‚Üí xAI
response = ask_llm(model_id="cualquiera", ...)  # Fallback transparente
```

### Rate Limit Detection
```python
# Detecci√≥n autom√°tica de l√≠mites de API
# Activaci√≥n de "candado de seguridad" con modelo local
```

### Circuit Breakers
- Evita cascadas de fallos
- Recovery autom√°tico tras per√≠odo de enfriamiento
- M√©tricas de salud por proveedor

## üìà Observabilidad

### Logging Estructurado
```python
logger.info(f"‚úÖ {provider.upper()} respondi√≥ exitosamente ({len(response)} caracteres)")
logger.warning(f"‚ùå {provider.upper()} fall√≥: {error_msg}")
logger.info(f"üéØ Respuesta recuperada de cache para tarea: {task_type}")
```

### M√©tricas Disponibles
- **Performance**: Latencia por proveedor, tokens/segundo
- **Costos**: Tokens utilizados, estimaci√≥n de costos por proveedor
- **Resiliencia**: Rate de fallos, fallbacks ejecutados, cache hits
- **Usage**: Distribuci√≥n de carga entre proveedores

## üîß Desarrollo y Extensi√≥n

### Agregar Nuevo Proveedor LLM

1. **Crear cliente en `api_clients.py`**:
```python
def call_nuevo_llm(messages: list) -> str:
    """Cliente para Nuevo Proveedor LLM"""
    # Implementaci√≥n espec√≠fica
    pass
```

2. **Registrar en `main_llm_service.py`**:
```python
elif provider_name == "nuevo":
    return lambda: call_nuevo_llm(messages)
```

3. **Actualizar configuraci√≥n**:
```bash
export NUEVO_API_KEY="tu-key"
export LLM_PRIORITY_ORDER="nuevo,gemini,local,..."
```

### Testing

```python
# Ejemplo de test
def test_ask_llm():
    response = ask_llm(
        model_id="ai/smollm3",
        system_prompt="Eres un asistente √∫til",
        user_prompt="Hola",
        task_type="simple_generation"
    )
    assert len(response) > 0
    assert "Error" not in response
```

## üèÜ Principios de Dise√±o Aplicados

### SOLID
- **S**: Cada servicio tiene una responsabilidad √∫nica
- **O**: Extensible para nuevos proveedores sin modificar c√≥digo existente
- **L**: Los clientes pueden sustituirse transparentemente
- **I**: Interfaces segregadas por funcionalidad
- **D**: Dependencias hacia abstracciones, no implementaciones concretas

### Patrones de Dise√±o
- **Strategy Pattern**: Selecci√≥n din√°mica de proveedores LLM
- **Chain of Responsibility**: Fallback entre proveedores
- **Singleton**: Cache y configuraci√≥n global
- **Factory**: Creaci√≥n de clientes LLM

## üöÄ Roadmap

### v2.1 (Pr√≥ximo)
- [ ] M√©tricas avanzadas con Prometheus
- [ ] Dashboard de monitoreo en tiempo real  
- [ ] API REST para gesti√≥n externa

### v2.2 (Futuro)
- [ ] Balanceador de carga inteligente
- [ ] Predicci√≥n de costos pre-ejecuci√≥n
- [ ] Optimizaci√≥n autom√°tica de prompts

---

**üß† DirGen Core - La inteligencia centralizada de la plataforma DirGen**

*Optimizaci√≥n ‚Ä¢ Resiliencia ‚Ä¢ Escalabilidad ‚Ä¢ Performance*